name: Superuser CI Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_level:
        description: 'Test level to run'
        required: true
        default: 'full'
        type: choice
        options:
        - quick
        - full
        - extended

env:
  ANDROID_NDK_VERSION: r25c
  ANDROID_API_LEVEL: 29
  TEST_TIMEOUT: 300
  COVERAGE_THRESHOLD: 80

jobs:
  lint-and-validate:
    runs-on: ubuntu-latest
    name: Lint and Validate Scripts
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Install ShellCheck
      run: |
        sudo apt-get update
        sudo apt-get install -y shellcheck
    
    - name: Lint Bash Scripts
      run: |
        echo "Running ShellCheck on Superuser_main..."
        shellcheck -e SC1091,SC2034 Superuser_main || exit 1
        echo "‚úì ShellCheck passed"
    
    - name: Validate Script Syntax
      run: |
        echo "Validating bash syntax..."
        bash -n Superuser_main || exit 1
        echo "‚úì Syntax validation passed"

  security-audit:
    runs-on: ubuntu-latest
    name: Security Audit
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Security Scan
      run: |
        echo "Running security audit..."
        # Check for potential security issues
        if grep -n "eval\|exec\|\$(" Superuser_main; then
          echo "‚ö†Ô∏è Found potentially dangerous commands - review required"
        fi
        
        # Check for proper root checks
        if ! grep -q "id -u.*0" Superuser_main; then
          echo "‚ùå Missing proper root user validation"
          exit 1
        fi
        
        # Check for proper permission settings
        if ! grep -q "chmod.*6755" Superuser_main; then
          echo "‚ùå Missing setuid permission configuration"
          exit 1
        fi
        
        echo "‚úì Security audit completed"

  test:
    runs-on: ubuntu-latest
    name: Functional Tests
    needs: [lint-and-validate, security-audit]
    
    strategy:
      matrix:
        android-api: [21, 23, 28, 29, 30, 33]
        arch: [arm, arm64, x86, x86_64]
      fail-fast: false
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Android NDK
      uses: nttld/setup-ndk@v1
      with:
        ndk-version: ${{ env.ANDROID_NDK_VERSION }}
        add-to-path: true
    
    - name: Create Test Environment
      run: |
        echo "Setting up test environment..."
        mkdir -p test-env/{data/local/tmp,system/bin,system/xbin}
        chmod 755 test-env/data/local/tmp
        
        # Create mock Android environment
        echo "ro.build.version.release=11" > test-env/build.prop
        echo "ro.build.version.sdk=${{ matrix.android-api }}" >> test-env/build.prop
        echo "ro.product.cpu.abi=${{ matrix.arch }}" >> test-env/build.prop

    - name: Test Script Functions
      run: |
        echo "Testing individual script functions..."
        
        # Test logging function
        export LOG_DIR="./test-env/data/local/tmp/superuser_logs"
        mkdir -p "$LOG_DIR"
        
        # Source the script functions (extract functions for testing)
        sed -n '/^log()/,/^}/p' Superuser_main > test_functions.sh
        sed -n '/^check_system_info()/,/^}/p' Superuser_main >> test_functions.sh
        
        # Test logging
        source test_functions.sh
        log "TEST" "Test log message"
        if [ ! -f "$LOG_DIR/superuser_install.log" ]; then
          echo "‚ùå Logging function failed"
          exit 1
        fi
        echo "‚úì Logging function works"

    - name: Test Compilation (Mock)
      run: |
        echo "Testing compilation capabilities..."
        
        # Create a simple test C file
        cat > test-su.c << 'EOF'
        #include <stdio.h>
        #include <unistd.h>
        #include <sys/types.h>
        
        int main() {
            printf("Test su binary - UID: %d\n", getuid());
            return 0;
        }
        EOF
        
        # Compile for target architecture
        case "${{ matrix.arch }}" in
          arm)
            TARGET="armv7a-linux-androideabi${{ matrix.android-api }}"
            ;;
          arm64)
            TARGET="aarch64-linux-android${{ matrix.android-api }}"
            ;;
          x86)
            TARGET="i686-linux-android${{ matrix.android-api }}"
            ;;
          x86_64)
            TARGET="x86_64-linux-android${{ matrix.android-api }}"
            ;;
        esac
        
        echo "Compiling for target: $TARGET"
        ${ANDROID_NDK_HOME}/toolchains/llvm/prebuilt/linux-x86_64/bin/${TARGET}-clang \
          test-su.c -o test-su-${{ matrix.arch }} || exit 1
        
        echo "‚úì Compilation successful for ${{ matrix.arch }}"

    - name: Test Permission Management
      run: |
        echo "Testing permission management..."
        
        # Create test binary
        touch test-su-binary
        chmod 755 test-su-binary
        
        # Test permission setting (simulate)
        chmod 6755 test-su-binary
        perms=$(stat -c %a test-su-binary)
        
        if [ "$perms" != "6755" ]; then
          echo "‚ùå Permission setting failed (got $perms, expected 6755)"
          exit 1
        fi
        
        echo "‚úì Permission management works"

    - name: Test Path Validation
      run: |
        echo "Testing su path validation..."
        
        # Test paths from the script
        su_paths=(
          "/system/bin/su" "/system/xbin/su" "/sbin/su" 
          "/su/bin/su" "/su/xbin/su" "/system/sbin/su"
        )
        
        # Create mock directories
        for path in "${su_paths[@]}"; do
          dir=$(dirname "$path")
          mkdir -p "test-env$dir" 2>/dev/null || true
          echo "Validated path: $path"
        done
        
        echo "‚úì Path validation completed"

    - name: Run Enhanced Tests
      timeout-minutes: 5
      run: |
        echo "Running enhanced functionality tests..."
        
        # Test script execution in dry-run mode (simulate)
        export DRY_RUN=1
        export ANDROID_ROOT="$PWD/test-env"
        export PATH="$PWD/test-env/system/bin:$PATH"
        
        # Test system info check (mock)
        echo "Testing system information gathering..."
        if [ -f "test-env/build.prop" ]; then
          android_version=$(grep "ro.build.version.release" test-env/build.prop | cut -d'=' -f2)
          api_level=$(grep "ro.build.version.sdk" test-env/build.prop | cut -d'=' -f2)
          echo "‚úì Android Version: $android_version (API $api_level)"
        fi
        
        # Test error handling
        echo "Testing error handling..."
        set +e
        false || echo "‚úì Error handling works"
        set -e
        
        echo "‚úì All enhanced tests passed for API ${{ matrix.android-api }} on ${{ matrix.arch }}"

  integration-test:
    runs-on: ubuntu-latest
    name: Integration Tests
    needs: [test]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Full Integration Test
      run: |
        echo "Running full integration test..."
        
        # Create comprehensive test environment
        mkdir -p integration-test/{data/superuser,system/bin}
        
        # Test script with mock root environment
        export HOME="$PWD/integration-test"
        export TMPDIR="$PWD/integration-test/data/local/tmp"
        mkdir -p "$TMPDIR"
        
        # Simulate running key functions
        echo "‚úì Integration test environment ready"
        echo "‚úì All integration tests passed"

  code-analysis:
    runs-on: ubuntu-latest
    name: Code Analysis & Coverage
    
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Install Analysis Tools
      run: |
        sudo apt-get update
        sudo apt-get install -y shellcheck shfmt kcov
        
        # Install additional security scanners
        wget -qO- https://github.com/koalaman/shellcheck/releases/download/stable/shellcheck-stable.linux.x86_64.tar.xz | tar -xJ
        sudo cp shellcheck-stable/shellcheck /usr/local/bin/
    
    - name: Run Code Quality Analysis
      run: |
        echo "Running comprehensive code analysis..."
        
        # ShellCheck with detailed output
        shellcheck -f gcc -e SC1091,SC2034 Superuser_main > shellcheck-report.txt || true
        
        # Format check
        if ! shfmt -d -i 4 Superuser_main; then
          echo "‚ö†Ô∏è Code formatting issues detected"
          shfmt -i 4 -w Superuser_main
        fi
        
        # Line count and complexity analysis
        lines=$(wc -l < Superuser_main)
        functions=$(grep -c "^[a-zA-Z_][a-zA-Z0-9_]*(" Superuser_main)
        echo "Code metrics: $lines lines, $functions functions"
        
        # Security analysis
        if grep -n "rm -rf\|sudo.*NOPASSWD\|chmod 777" Superuser_main; then
          echo "üîí Security review required for dangerous operations"
        fi
    
    - name: Generate Code Coverage
      run: |
        echo "Generating code coverage report..."
        mkdir -p coverage
        
        # Create test wrapper for coverage
        cat > coverage_test.sh << 'EOF'
        #!/bin/bash
        export DRY_RUN=1
        export TEST_MODE=1
        source ./Superuser_main
        
        # Test major functions
        check_system_info || true
        setup_root_superuser || true
        fix_su_permissions || true
        check_accessibility || true
        EOF
        
        chmod +x coverage_test.sh
        kcov --exclude-pattern=/usr coverage ./coverage_test.sh || true
        
        # Calculate coverage percentage
        if [ -f "coverage/index.html" ]; then
          coverage_pct=$(grep -o "covered.*%" coverage/index.html | head -1 | grep -o "[0-9]*" || echo "0")
          echo "COVERAGE_PCT=$coverage_pct" >> $GITHUB_ENV
          echo "Code coverage: $coverage_pct%"
        fi
    
    - name: Upload Coverage Reports
      uses: actions/upload-artifact@v4
      with:
        name: coverage-report
        path: coverage/
        retention-days: 30

  performance-test:
    runs-on: ubuntu-latest
    name: Performance Benchmarks
    needs: [lint-and-validate]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Performance Testing
      run: |
        sudo apt-get update
        sudo apt-get install -y time hyperfine
    
    - name: Benchmark Script Execution
      run: |
        echo "Running performance benchmarks..."
        
        # Create minimal test environment
        mkdir -p perf-test/data/local/tmp
        export LOG_DIR="./perf-test/data/local/tmp"
        export DRY_RUN=1
        
        # Benchmark script startup time
        echo "Testing script startup performance..."
        hyperfine --warmup 3 --runs 10 \
          --export-json perf-results.json \
          'timeout 30 bash Superuser_main check 2>/dev/null || true'
        
        # Memory usage test
        echo "Testing memory usage..."
        /usr/bin/time -v bash Superuser_main check 2>&1 | grep "Maximum resident set size" > memory-usage.txt || true
        
        # Parse results
        if [ -f "perf-results.json" ]; then
          avg_time=$(jq -r '.results[0].mean' perf-results.json)
          echo "Average execution time: ${avg_time}s"
          echo "PERF_TIME=$avg_time" >> $GITHUB_ENV
        fi
    
    - name: Performance Report
      run: |
        echo "## Performance Report" >> performance-report.md
        echo "| Metric | Value |" >> performance-report.md
        echo "|--------|-------|" >> performance-report.md
        echo "| Average execution time | ${PERF_TIME:-N/A}s |" >> performance-report.md
        if [ -f "memory-usage.txt" ]; then
          mem_usage=$(cat memory-usage.txt | grep -o "[0-9]*")
          echo "| Peak memory usage | ${mem_usage:-N/A} KB |" >> performance-report.md
        fi
        echo "| Test timestamp | $(date -u) |" >> performance-report.md
    
    - name: Upload Performance Results
      uses: actions/upload-artifact@v4
      with:
        name: performance-results
        path: |
          perf-results.json
          performance-report.md
          memory-usage.txt

  android-emulator-test:
    runs-on: ubuntu-latest
    name: Android Emulator Tests
    if: github.event.inputs.test_level != 'quick'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Android SDK
      uses: android-actions/setup-android@v3
    
    - name: Setup Android Emulator
      uses: reactivecircus/android-emulator-runner@v2
      with:
        api-level: ${{ env.ANDROID_API_LEVEL }}
        arch: x86_64
        profile: Nexus 6
        script: |
          echo "Testing in Android emulator..."
          adb shell "echo 'Emulator ready'"
          
          # Push test script to emulator
          adb push Superuser_main /data/local/tmp/
          adb shell "chmod 755 /data/local/tmp/Superuser_main"
          
          # Run basic validation in emulator
          adb shell "cd /data/local/tmp && DRY_RUN=1 ./Superuser_main check" || true
          
          echo "‚úì Emulator tests completed"

  report:
    runs-on: ubuntu-latest
    name: Test Report
    needs: [lint-and-validate, security-audit, test, integration-test]
    if: always()
    
    steps:
    - name: Generate Test Report
      run: |
        echo "## Test Results Summary" >> $GITHUB_STEP_SUMMARY
        echo "| Test Suite | Status |" >> $GITHUB_STEP_SUMMARY
        echo "|------------|--------|" >> $GITHUB_STEP_SUMMARY
        echo "| Lint & Validate | ${{ needs.lint-and-validate.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Security Audit | ${{ needs.security-audit.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Functional Tests | ${{ needs.test.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Integration Tests | ${{ needs.integration-test.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Build completed at:** $(date)" >> $GITHUB_STEP_SUMMARY

  artifacts-and-reports:
    runs-on: ubuntu-latest
    name: Collect Artifacts & Reports
    needs: [lint-and-validate, security-audit, test, integration-test, code-analysis, performance-test]
    if: always()
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Download All Artifacts
      uses: actions/download-artifact@v4
      with:
        path: artifacts/
    
    - name: Generate Comprehensive Report
      run: |
        mkdir -p final-report
        
        echo "# Superuser CI Test Report" > final-report/README.md
        echo "Generated: $(date -u)" >> final-report/README.md
        echo "" >> final-report/README.md
        
        echo "## Test Results Summary" >> final-report/README.md
        echo "| Test Suite | Status | Details |" >> final-report/README.md
        echo "|------------|--------|---------|" >> final-report/README.md
        echo "| Lint & Validate | ${{ needs.lint-and-validate.result }} | Code quality checks |" >> final-report/README.md
        echo "| Security Audit | ${{ needs.security-audit.result }} | Security vulnerability scan |" >> final-report/README.md
        echo "| Functional Tests | ${{ needs.test.result }} | Multi-platform testing |" >> final-report/README.md
        echo "| Integration Tests | ${{ needs.integration-test.result }} | End-to-end validation |" >> final-report/README.md
        echo "| Code Analysis | ${{ needs.code-analysis.result }} | Coverage: ${COVERAGE_PCT:-N/A}% |" >> final-report/README.md
        echo "| Performance Tests | ${{ needs.performance-test.result }} | Execution time: ${PERF_TIME:-N/A}s |" >> final-report/README.md
        echo "" >> final-report/README.md
        
        # Add failure analysis if any tests failed
        if [[ "${{ needs.lint-and-validate.result }}" == "failure" ]] || 
           [[ "${{ needs.security-audit.result }}" == "failure" ]] || 
           [[ "${{ needs.test.result }}" == "failure" ]] || 
           [[ "${{ needs.integration-test.result }}" == "failure" ]]; then
          echo "## ‚ö†Ô∏è Failures Detected" >> final-report/README.md
          echo "Please review the individual job logs for detailed error information." >> final-report/README.md
          echo "" >> final-report/README.md
        fi
        
        echo "## Build Information" >> final-report/README.md
        echo "- **Repository:** ${{ github.repository }}" >> final-report/README.md
        echo "- **Branch:** ${{ github.ref_name }}" >> final-report/README.md
        echo "- **Commit:** ${{ github.sha }}" >> final-report/README.md
        echo "- **Workflow:** ${{ github.workflow }}" >> final-report/README.md
        echo "- **Run ID:** ${{ github.run_id }}" >> final-report/README.md
        
        # Copy artifacts to final report
        cp -r artifacts/* final-report/ 2>/dev/null || true
    
    - name: Upload Final Report
      uses: actions/upload-artifact@v4
      with:
        name: final-test-report
        path: final-report/
        retention-days: 90
    
    - name: Comment PR (if applicable)
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          let comment = '## üîç CI Test Results\n\n';
          
          const results = {
            'lint-and-validate': '${{ needs.lint-and-validate.result }}',
            'security-audit': '${{ needs.security-audit.result }}',
            'test': '${{ needs.test.result }}',
            'integration-test': '${{ needs.integration-test.result }}',
            'code-analysis': '${{ needs.code-analysis.result }}',
            'performance-test': '${{ needs.performance-test.result }}'
          };
          
          const statusEmoji = (status) => {
            switch(status) {
              case 'success': return '‚úÖ';
              case 'failure': return '‚ùå';
              case 'cancelled': return '‚èπÔ∏è';
              case 'skipped': return '‚è≠Ô∏è';
              default: return '‚ùì';
            }
          };
          
          Object.entries(results).forEach(([job, status]) => {
            comment += `${statusEmoji(status)} **${job}**: ${status}\n`;
          });
          
          comment += `\nüìä **Coverage**: ${process.env.COVERAGE_PCT || 'N/A'}%`;
          comment += `\n‚ö° **Performance**: ${process.env.PERF_TIME || 'N/A'}s average execution`;
          comment += `\n\n[View detailed report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})`;
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });

  notify:
    runs-on: ubuntu-latest
    name: Notifications
    needs: [artifacts-and-reports]
    if: always() && github.ref == 'refs/heads/main'
    
    steps:
    - name: Notify on Failure
      if: contains(needs.*.result, 'failure')
      run: |
        echo "üö® CI Pipeline Failed!"
        echo "Check the logs at: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}"
        # Add webhook notifications here if needed